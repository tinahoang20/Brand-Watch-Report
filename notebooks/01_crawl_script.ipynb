# Expanded web crawling script for Facebook, Instagram, and YouTube

import os
import time
import pandas as pd
from datetime import datetime, timedelta
from facebook_scraper import get_posts
from instaloader import Instaloader, Profile
from googleapiclient.discovery import build

# === CONFIGURATION ===
BRANDS = {
    'Coke': {
        'facebook': 'TCCCVN',
        'instagram': 'cocacolavietnam',
        'youtube': 'UCpF8WgFJ9mIGB4gT6KWuSXg'
    },
    'Pepsi': {
        'facebook': 'Pepsivietnam',
        'instagram': 'pepsivietnam',
        'youtube': 'UCYIgiIYJYSt7zcuD85hTAXA'
    },
    'Fanta': {
        'facebook': 'fantavietnam',
        'instagram': 'fantavietnam',
        'youtube': 'UC65Fg9P7DOpL_UOopzJLZcA'
    }
}

START_DATE = datetime(2024, 11, 1)
END_DATE = datetime(2025, 3, 31)

# === FACEBOOK CRAWLING ===
def crawl_facebook_page(page_name, brand_name):
    posts = []
    print(f"[Facebook] Crawling {brand_name} - {page_name}")
    for post in get_posts(page_name, pages=50, options={"comments": False, "reactors": False}):
        post_time = post['time']
        if START_DATE <= post_time <= END_DATE:
            posts.append(post)
        if post_time < START_DATE:
            break
    return pd.DataFrame(posts)

# === INSTAGRAM CRAWLING ===
def crawl_instagram_profile(username):
    print(f"[Instagram] Crawling @{username}")
    loader = Instaloader()
    profile = Profile.from_username(loader.context, username)
    posts = []
    for post in profile.get_posts():
        post_date = post.date
        if START_DATE <= post_date <= END_DATE:
            posts.append({
                'date': post.date,
                'caption': post.caption,
                'likes': post.likes,
                'comments': post.comments,
                'url': post.url
            })
        elif post_date < START_DATE:
            break
    return pd.DataFrame(posts)

# === YOUTUBE CRAWLING ===
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
def crawl_youtube_channel(channel_id):
    print(f"[YouTube] Crawling Channel ID: {channel_id}")
    youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
    videos = []
    search_response = youtube.search().list(
        channelId=channel_id,
        part='snippet',
        maxResults=50,
        order='date'
    ).execute()

    for item in search_response['items']:
        video_id = item['id'].get('videoId')
        if not video_id:
            continue

        video_date = item['snippet']['publishedAt']
        video_date = datetime.strptime(video_date, '%Y-%m-%dT%H:%M:%SZ')
        if START_DATE <= video_date <= END_DATE:
            videos.append({
                'video_id': video_id,
                'title': item['snippet']['title'],
                'description': item['snippet']['description'],
                'published_at': video_date
            })
        elif video_date < START_DATE:
            break

    return pd.DataFrame(videos)

# === MAIN PROCESS ===
facebook_dfs = []
instagram_dfs = []
youtube_dfs = []

for brand, ids in BRANDS.items():
    fb_df = crawl_facebook_page(ids['facebook'], brand)
    fb_df['brand'] = brand
    facebook_dfs.append(fb_df)

    insta_df = crawl_instagram_profile(ids['instagram'])
    insta_df['brand'] = brand
    instagram_dfs.append(insta_df)

    yt_df = crawl_youtube_channel(ids['youtube'])
    yt_df['brand'] = brand
    youtube_dfs.append(yt_df)

# SAVE TO FILES
pd.concat(facebook_dfs).to_csv('data/facebook_posts.csv', index=False)
pd.concat(instagram_dfs).to_csv('data/instagram_posts.csv', index=False)
pd.concat(youtube_dfs).to_csv('data/youtube_videos.csv', index=False)

print("[INFO] Done collecting all data.")
